Consider the general MIMO communication system given by 
	\begin{equation}\label{com_system}
	\vy = \mH_{Total}\vx + \vn
	\end{equation}
	in which $\mH_{Total}$ is a random channel (we can always represent linearly?).
A key metric of interest for this system is the mutual information between the transmitted and received signal given by 
\begin{equation}\label{mut_ent}
I(\vy;\vx) = h(\vy) - h(\vy|\vx).
\end{equation}
If both $\vx$ and $\vn$ are circularly symmetric Gaussian random vectors, $\vy$ is also circularly symmetric Gaussian with entropy given by \cite{telatar1999capacity} 
\begin{equation}\label{entropy}
h(\vy) = \Expect[\Log(|\pi e \mathbf{Q}_{y}|)]
\end{equation}
 for $\mathbf{Q}_{y} = \Expect{[\vy \vy^H]}$. Note also that for a given $\mathbf{Q}_{y}$ the entropy is maximized if and only if $\vy$ is circularly symmetric Gaussian. (explain what implication this has)
For a given transmit covariance matrix, $\mathbf{Q_x}$ and $\Expect\left[\vn\vn^H \right] = \mI$, substituting equation \eqref{entropy} into equation
\eqref{mut_ent} and simplifying gives
\begin{equation}\label{mut_inf}
\Expect \left[\Log(|\mathbf{I} + \mHt \mQ_x \mHt^H|)\right].
\end{equation}
Maximizing the mutual informationwith respect to both the
transmit covariance matrix and phases at the RIS gives an expression for the system capacity

\begin{equation}\label{capacity}
\mathbb{C} = \underset{\boldsymbol{\Phi},\mathbf{Q_x}}{\mathop{max}} \Expect \left[\Log(|\mathbf{I} + \mHt \mQ_x \mHt^H|)\right]
\end{equation}
or equivalently,
\begin{equation}\label{capacity_tricky}
\mathbb{C} = \underset{\boldsymbol{\Phi},\mathbf{Q_x}}{\mathop{max}} \Expect \left[\sum_{i=1}^{N}\Log(1 + \lambda_i)\right].
\end{equation}
where $\lambda_i$ are the eigenvalues of $\mHt \mQ_x \mHt^H$.

Recalling that in general we are interested in evaluating the capacity in order to determine a feasible transmission rate for a given communication channel, equation \eqref{capacity_tricky} poses a couple of hurdles to evaluation. First, we need to be able to evaluate an expectation over the joint pdf $p(\lambda_1 \cdots	 \lambda_N)$. Second, there is an optimization problem whose solution may in general depend on the distribution of the channel. Including this solution in the capacity expression without a closed form may be difficult.
To resolve these hurdles, two simplifications will be used.
First, we will enforce that the transmitter chooses a deterministic $\mQ_x$. 
By enforcing a deterministic choice of $\mQ_x$, we no longer have to consider this component of the expectation.
Second we will normalize the channel, and therefore the received power, to allow the received signal covariance matrix eigenvalue distribution to converge. 

If $\mHt$ is not known at the transmitter, an intuitive choice for the transmitter signal covariance matrix is  $\mQ_x = \frac{P_{\text{total}}}{N_t}\mathbf{I}$ which leads to the form of equation \eqref{capacity}
\begin{equation}\label{no_csi_capacity}
\mathbb{C} = \underset{\phase}{\mathop{max}} \; N_R \Expect \left[\Log(1 + \frac{P_{\text{total}}}{\sigma_nN_t}\lambda_i)\right].
\end{equation}
where $\lambda_i$ are the eigenvalues of $\mHt \mHt^H$.
In this form, we can clearly see the linear scaling of capacity with $N_R$ that has motivated MIMO research.
If we assume that $\Expect\left[ \vy \vy^H \right]$ has a converging eigenvalue distribution as the system dimensions increase to infinity, we can expand the expectation over the AED to get the expression
\begin{equation}\label{no_csi_capacity_aed}
\mathbb{C} = \underset{\phase}{\mathop{max}} \; N_R  \int_{0}^{\infty}\Log(1 + \frac{1}{\sigma_n}x)p_{\lambda\lambda^H}(x) dx
\end{equation}
in which $p_{\lambda\lambda^H}(x)$ is the channel AED and the bounds of the integral reflect the range for the eigenvalues of a positive semidefinite matrix.
Note that because the choice for the distributions of $\vx$ and $\vn$ maximize $h(\vy)$, equation \eqref{capacity} provides an upper bound on the capacity of the channel for any choices of the distribution of $\vx $ with Gaussian noise $\vn$. (Perhaps mention that normally we cannot generate continuous Gaussian x so we use channel coding).


Note that because $\Log$ is a concave function, we can upper bound the mutual information with
\begin{equation}\label{mut_inf}
\Expect[\Log(1 + \frac{1}{\sigma_n}\lambda)] \leq \Log(1 + \frac{1}{\sigma_n}\Expect[\lambda])
\end{equation}